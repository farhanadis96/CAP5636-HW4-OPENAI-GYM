{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Definition\n",
    "The goal of this policy is to use the momentum of the mountain car to push it to the hill top without any external acceleration. That is, the policy has to drive the car in the same direction as the velocity. State 0 represents the position of the car with respect to the hill top. State 1 represents the velocity of the car obtained due to the momentum built.If the car's position is to the left side with respect to the hill top by 0.7, then the policy implmented here will drive the car 2 units towards the hill top, otherwise the car continues to remain in its current postion. If the car's velocity is positive, then again the policy will push the car 2 units towards the hill top, otherwise no step is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state[1] > 0:\n",
    "            step = 2\n",
    "    elif state[1] < 0:\n",
    "        step = 0\n",
    "    \n",
    "    elif state[0] < -0.7:\n",
    "        step = 2\n",
    "    else:\n",
    "        step = 0\n",
    "    \n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_total=[]\n",
    "for i_episode in range(20):\n",
    "    state = env.reset()\n",
    "\n",
    "    count=0\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        action=policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        count=count+reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "\n",
    "            break\n",
    "    reward_total.append(count)\n",
    "\n",
    "reward_total=np.array(reward_total)\n",
    "print(\"Rewards collected :\",reward_total)\n",
    "print(\"Maximum \",np.max(reward_total),\"Minimum \" ,np.min(reward_total),\"Mean \" ,np.mean(reward_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
