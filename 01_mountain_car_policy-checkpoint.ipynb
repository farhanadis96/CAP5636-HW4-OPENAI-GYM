{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Definition\n",
    "The goal of this policy is to use the momentum of the mountain car to push it to the hill top without any external acceleration. That is, the policy has to drive the car in the same direction as the velocity. State 0 represents the position of the car with respect to the hill top. State 1 represents the velocity of the car obtained due to the momentum built.If the car's position is to the left side with respect to the hill top by 0.7, then the policy implmented here will drive the car 2 units towards the hill top, otherwise the car continues to remain in its current postion. If the car's velocity is positive, then again the policy will push the car 2 units towards the hill top, otherwise no step is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state[1] > 0:\n",
    "            step = 2\n",
    "    elif state[1] < 0:\n",
    "        step = 0\n",
    "    \n",
    "    elif state[0] < -0.7:\n",
    "        step = 2\n",
    "    else:\n",
    "        step = 0\n",
    "    \n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 178 timesteps\n",
      "Episode finished after 93 timesteps\n",
      "Episode finished after 152 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 92 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 101 timesteps\n",
      "Episode finished after 95 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 116 timesteps\n",
      "Episode finished after 157 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 95 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 89 timesteps\n",
      "Episode finished after 95 timesteps\n",
      "Episode finished after 159 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Rewards collected : [-178.  -93. -152.  -88.  -92. -154. -156. -101.  -95.  -88. -116. -157.\n",
      " -164. -151.  -95. -151.  -89.  -95. -159.  -90.]\n",
      "Maximum  -88.0 Minimum  -178.0 Mean  -123.2\n"
     ]
    }
   ],
   "source": [
    "reward_total=[]\n",
    "for i_episode in range(20):\n",
    "    state = env.reset()\n",
    "\n",
    "    count=0\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        action=policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        count=count+reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "\n",
    "            break\n",
    "    reward_total.append(count)\n",
    "\n",
    "reward_total=np.array(reward_total)\n",
    "print(\"Rewards collected :\",reward_total)\n",
    "print(\"Maximum \",np.max(reward_total),\"Minimum \" ,np.min(reward_total),\"Mean \" ,np.mean(reward_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
